![1paper1week](../../docs/1paper1week-git.jpg)

# XGBoost algoritnm details notes

详细梳理XGBoost的算法细节，并进行整理。文章更多的目的是自用，如果您要参考，这里假设您已经基本上了解了 decision tree, gradient boost 以及一些机器学习的基础知识点。

## 1.模型原理

在树模型中，一个node拿到一堆数据，node首先要确定一个输出值$O_{\nu alue}$使得该节点下所有数据的Loss最小化，即：

$$
[\sum_{i=1}^nL(y_i,p_i^0+O_{\nu alue})]+\frac{1}{2}\lambda O_{\nu alue}^2
$$

XGBoost使用taylor展开来寻找上述函数的极值点，即上式等于：

$$L(y,p_{i}+O_{value})\approx L(y,p_{i})+gO_{value}+\frac{1}{2}hO_{value}^{2}$$

则对于该节点下的n个样本数据，上式展开后为：

$$\begin{aligned}
 & L(y_{1},p_{1}^{0})+g_{1}O_{\nu alue}+\frac{1}{2}h_{1}O_{\nu alue}^{2} \\
 & +L(y_{2},p_{2}^{0})+g_{2}O_{\nu alue}+\frac{1}{2}h_{2}O_{\nu alue}^{2}+\cdots+ \\
 & +L(y_{n},p_{n}^{0})+g_{n}O_{\nu alue}+\frac{1}{2}h_{n}O_{\nu alue}^{2}+\frac{1}{2}\lambda O_{\nu alue}^{2}
\end{aligned}$$

因为 $L(y,p_{i})$ 是常数项对最小化没有影响，所以上式变为：

$$(g_1+g_2+\cdots+g_n)O_{\nu alue}+\frac{1}{2}(h_1+h_2+\cdots+h_n+\lambda)O_{\nu alue}^2$$

求导并且设导数为零，求得：

$$O_{\nu alue}=\frac{-(g_1+g_2+\cdots+g_n)}{(h_1+h_2+\cdots+h_n+\lambda)}$$

注意，${(h_1+h_2+\cdots+h_n)}$ 其实就是这里所有数据的cover，同时与模型参数 min_child_weight 有关。

## 2.模型计算优化

### 2.1. Approximate Greedy Algo

XGBoost提供了贪心准则的近似版本，简言之，将特征分位数作为划分候选点。 这样将划分候选点集合由全样本间的遍历缩减到了几个分位数之间的遍历。

### 2.2. Weighted Quantile sketch

在划分分位的时候，模型会尽可能使得每个分位间包含的样本的cover一致。在MSE作为loss的回归任务中，这里cover就是样本数量；但在 cross entropy 的分类问题中，这里的cover并不是，因为这种情况下的二阶导为 （1-p）*p。也就是说之前的模型越不确定某些数据该如何分类，这里的近似算法越会分配给数据更高的weights。


### 2.3. Sparsity-Aware Split finding

当样本的第i个特征值缺失时，无法利用该特征进行划分时，XGBoost的想法是先使用剔除缺失样本的数据正常构建树，然后尝试将包含缺失数据的所有样本（作为一个完整的类）分别划分到左结点和右结点，然后计算其增益，哪个大就划分到哪边。后续在推理任务的过程中，i特征的缺失值一律遵循训练中的划分原则。


## 3. Feature Importance

XGboost内置了 plot_importance 的算法。可以使用 weight cover gain 三种衡量标准中的一个。但三种办法生成的[结果并不一致](https://www.jiqizhixin.com/articles/2020-09-23)

### 3.1. SHAP


**SHAP (SHapley Additive exPlanations) 是什么？**

SHAP 是一种博弈论方法，用于解释任何机器学习模型的输出。它的核心思想是：模型对某个样本的预测值，可以看作是所有特征共同作用的结果。SHAP值则衡量了每个特征对这个预测结果的“贡献”有多大。

想象一个合作游戏，团队完成了一个项目并获得了奖励。SHAP值就像是根据每个成员在项目中的贡献大小，来公平地分配这个奖励。在XGBoost中，“团队”就是所有的特征，“项目”就是做出一个预测，“奖励”就是预测值与基准值（例如，所有样本的平均预测值）之间的差异。

**SHAP 值是如何计算的？**

对于像XGBoost这样的树模型，SHAP值的计算有一种高效的算法叫做 TreeSHAP。其基本思想是：

1.  **基准值 (Base Value)**：首先，我们会有一个基准预测值。这通常是模型在没有任何特征信息时给出的预测，或者是在训练数据集上的平均预测值。
2.  **特征的边际贡献**：然后，SHAP会考虑当一个特征被加入到模型中（或者从模型中移除）时，预测结果会发生多大的变化。关键在于，它会考虑所有可能的特征排列组合顺序。
    *   例如，有特征 A, B, C。SHAP会考虑：
        *   单独加入 A 时的贡献。
        *   先加入 A，再加入 B 时，B 的贡献。
        *   先加入 B，再加入 A 时，A 的贡献。
        *   等等，所有可能的顺序。
3.  **Shapley 值**：对于每个特征，将其在所有可能排列组合下的边际贡献进行加权平均，就得到了该特征的SHAP值。这个值表示该特征将预测结果从基准值“推向”最终预测值的贡献量。

所以，对于一个特定的预测，每个特征都会有一个SHAP值。如果某个特征的SHAP值为正，说明该特征将预测结果向正方向推动（例如，预测为“是”的概率增加）；如果为负，则向负方向推动。所有特征的SHAP值之和，加上基准值，就等于模型对该样本的最终预测值。

**SHAP 值如何用于特征选择？**

SHAP值为特征选择提供了一个非常强大和可靠的依据：

1.  **全局特征重要性**：
    *   计算数据集中每个样本的每个特征的SHAP值。
    *   然后，对于每个特征，计算其SHAP值的**平均绝对值**（`mean(|SHAP value|)`）。
    *   这个平均绝对SHAP值越大的特征，说明它对模型的预测结果的整体影响力越大，因此也越重要。
    *   你可以根据这个重要性对特征进行排序，选择排名靠前的N个特征，或者设定一个阈值来筛选特征。

2.  **理解特征如何影响预测**：
    *   SHAP不仅告诉我们哪些特征重要，还能告诉我们特征是如何影响预测的。例如，通过SHAP的汇总图（summary plot），我们可以看到一个特征值较高时，是倾向于提高预测值还是降低预测值。


